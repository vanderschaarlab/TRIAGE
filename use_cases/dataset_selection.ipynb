{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U6VQ1jn4v3J3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sdv.tabular import CTGAN, TVAE\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.dataloader import *\n",
    "from src.models import *\n",
    "from triage.triage import Triage\n",
    "from src.utils import *\n",
    "\n",
    "_ALL_REGRESSION_DATASETS = ALL_REGRESSION_DATASETS\n",
    "\n",
    "\n",
    "nest = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7e9IssalLdmE"
   },
   "outputs": [],
   "source": [
    "uci_datasets = [\n",
    "    \"boston\",\n",
    "    \"star\",\n",
    "    \"bio\",\n",
    "    \"concrete\",\n",
    "    \"protein\",\n",
    "    \"bike\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CVvXItBxLRjy"
   },
   "outputs": [],
   "source": [
    "def get_uci(dataset):\n",
    "\n",
    "    if dataset in uci_datasets:\n",
    "        regression_datasets = list(_ALL_REGRESSION_DATASETS.keys())\n",
    "        regression_datasets.sort()\n",
    "\n",
    "        if dataset in regression_datasets:\n",
    "            # xlrd, openpyxl\n",
    "            import tempfile\n",
    "\n",
    "            dataset_name = regression_datasets[mapper[dataset]]\n",
    "            with tempfile.TemporaryDirectory() as data_dir:\n",
    "                # download and load data\n",
    "                download_regression_dataset(dataset_name, data_dir)\n",
    "                X, y = load_regression_dataset(\n",
    "                    dataset_name, data_dir, shuffle_train=True, batch_size=512\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            X, y = GetDataset(dataset, \"./data/\")\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hyperimpute.plugins.imputers import Imputers\n",
    "\n",
    "\n",
    "def get_medical(dataset=\"mimic_antibiotics\", processing_needed=False, seed=42):\n",
    "\n",
    "\n",
    "    if dataset == \"mimic_antibiotics\":\n",
    "        outcome_variable = \"wbc\"\n",
    "        df_static = pd.read_csv(\"data/mimic_antibiotics_static_train_data.csv\")\n",
    "        df_temporal = pd.read_csv(\"data/mimic_antibiotics_temporal_train_data_eav.csv\")\n",
    "        processing_needed = True\n",
    "\n",
    "\n",
    "    if dataset == \"los\":\n",
    "        outcome_variable = \"lengthofstay\"\n",
    "        df = pd.read_csv(\"data/LengthOfStay.csv\")\n",
    "        dropcols = [\"eid\", \"vdate\", \"discharged\", \"facid\"]\n",
    "        df = df.drop(columns=dropcols)\n",
    "        df = df.sample(n=10000)\n",
    "\n",
    "        from sklearn import preprocessing\n",
    "\n",
    "        le1 = preprocessing.LabelEncoder()\n",
    "        df[\"rcount\"] = le1.fit_transform(df[\"rcount\"])\n",
    "        le2 = preprocessing.LabelEncoder()\n",
    "        df[\"gender\"] = le2.fit_transform(df[\"gender\"])\n",
    "        processing_needed = False\n",
    "\n",
    "    if dataset != \"cancer\" and dataset not in uci_datasets:\n",
    "        if processing_needed:\n",
    "            ids_static = df_static.id.unique()\n",
    "            ids_temporal = df_temporal[df_temporal[\"time\"] == 0].id.unique()\n",
    "\n",
    "            if len(ids_temporal) < len(ids_static):\n",
    "                ids = ids_temporal\n",
    "            else:\n",
    "                ids = ids_static\n",
    "\n",
    "            df_static = df_static[df_static.id.isin(ids)]\n",
    "            df_temporal = df_temporal[df_temporal.id.isin(ids)]\n",
    "            df_temporal = df_temporal[df_temporal[\"time\"] == 0]\n",
    "\n",
    "            df_temporal_label = df_temporal[df_temporal[\"variable\"] == outcome_variable]\n",
    "            if len(df_temporal_label) == 0:\n",
    "                df_temporal_label = df_static[[outcome_variable, \"id\"]]\n",
    "\n",
    "            if len(df_temporal_label) != len(df_temporal):\n",
    "                ids = df_temporal_label.id\n",
    "                df_static = df_static[df_static.id.isin(ids)]\n",
    "                df_temporal = df_temporal[df_temporal.id.isin(ids)]\n",
    "\n",
    "            df_static.shape, df_temporal[\n",
    "                df_temporal[\"variable\"] == outcome_variable\n",
    "            ].shape\n",
    "\n",
    "            df_temporal = df_temporal.sort_values(by=[\"id\"])\n",
    "            df_static = df_static.sort_values(by=[\"id\"])\n",
    "\n",
    "            temporal_unique_ids = df_temporal.id.unique().shape\n",
    "            static_shape = df_static.shape\n",
    "   \n",
    "            assert temporal_unique_ids[0] == static_shape[0]\n",
    "\n",
    "            feature_dict = {}\n",
    "            for variable_name in list(df_temporal.variable.unique()):\n",
    "                tdf = df_temporal[df_temporal[\"variable\"] == variable_name]\n",
    "                variable_array = tdf.drop_duplicates(subset=[\"id\"]).value.to_numpy()\n",
    "                feature_dict[variable_name] = variable_array\n",
    "\n",
    "            tmp_df = pd.DataFrame.from_dict(\n",
    "                feature_dict, columns=df_static.id, orient=\"index\"\n",
    "            ).T.reset_index(level=0)\n",
    "         \n",
    "\n",
    "            tmp_df.shape\n",
    "            df_overall = df_static.merge(tmp_df, on=\"id\", how=\"left\")\n",
    "\n",
    "            assert df_overall[outcome_variable].shape[0] == static_shape[0]\n",
    "\n",
    "        else:\n",
    "            df_overall = df\n",
    "\n",
    "    ###########################################\n",
    "    # DATA SETUP\n",
    "    ###########################################\n",
    "    if dataset != \"cancer\":\n",
    "        print(len(df_overall), dataset)\n",
    "        cols = df_overall.columns\n",
    "        if len(df_overall) > 10000:\n",
    "            df_overall = df_overall.sample(n=10000)\n",
    "\n",
    "        from sklearn.impute import SimpleImputer\n",
    "\n",
    "        imp_mean = SimpleImputer(strategy=\"mean\")\n",
    "        df_overall = imp_mean.fit_transform(df_overall)\n",
    "        df_overall = pd.DataFrame(df_overall)\n",
    "        df_overall.columns = cols\n",
    "\n",
    "        X = df_overall.drop(columns=[outcome_variable])\n",
    "        y = df_overall[outcome_variable].values\n",
    "\n",
    "    if dataset == \"cancer\":\n",
    "        X, y, df = load_seer_cutract_dataset(name=\"seer\", seed=seed)\n",
    "        y = y.values\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "y8146c5yL1y9"
   },
   "outputs": [],
   "source": [
    "mapper = {}\n",
    "for i, val in enumerate(regression_datasets):\n",
    "    mapper[val] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DMFR2MjwWnu",
    "outputId": "b86182d8-1025-4f30-9c6b-dfad11bcf1c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike\n",
      "CTGAN 0\n",
      "[Errno 2] No such file or directory: './data/bike_train.csv'\n",
      "boston\n",
      "CTGAN 0\n",
      "CTGAN 1\n",
      "CTGAN 2\n",
      "CTGAN 3\n",
      "CTGAN 4\n",
      "CTGAN\n",
      "TVAE 0\n",
      "TVAE 1\n",
      "TVAE 2\n",
      "TVAE 3\n",
      "TVAE 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [01:52<06:32, 56.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVAE\n",
      "boston\n",
      "{'CTGAN': {'discard': 0.59248, 'mae': 0.16940937866560452}, 'TVAE': {'discard': 0.40671999999999997, 'mae': 0.12299813702623756}}\n",
      "concrete\n",
      "CTGAN 0\n",
      "CTGAN 1\n",
      "CTGAN 2\n",
      "CTGAN 3\n",
      "CTGAN 4\n",
      "CTGAN\n",
      "TVAE 0\n",
      "TVAE 1\n",
      "TVAE 2\n",
      "TVAE 3\n",
      "TVAE 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [04:22<09:32, 95.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVAE\n",
      "concrete\n",
      "{'CTGAN': {'discard': 0.5768600000000002, 'mae': 0.18450783407877333}, 'TVAE': {'discard': 0.5128600000000001, 'mae': 0.11010332626540328}}\n",
      "star\n",
      "CTGAN 0\n",
      "[Errno 2] No such file or directory: './data/STAR.csv'\n",
      "mimic_antibiotics\n",
      "CTGAN 0\n",
      "[Errno 2] No such file or directory: 'data/mimic_antibiotics_static_train_data.csv'\n",
      "los\n",
      "CTGAN 0\n",
      "[Errno 2] No such file or directory: 'data/LengthOfStay.csv'\n",
      "cancer\n",
      "CTGAN 0\n",
      "CTGAN 1\n",
      "CTGAN 2\n",
      "CTGAN 3\n",
      "CTGAN 4\n",
      "CTGAN\n",
      "TVAE 0\n",
      "TVAE 1\n",
      "TVAE 2\n",
      "TVAE 3\n",
      "TVAE 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [21:36<07:00, 210.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVAE\n",
      "cancer\n",
      "{'CTGAN': {'discard': 0.51586, 'mae': 0.19781326705459215}, 'TVAE': {'discard': 0.50706, 'mae': 0.182426155038921}}\n",
      "protein\n",
      "CTGAN 0\n",
      "CTGAN 1\n",
      "CTGAN 2\n",
      "CTGAN 3\n",
      "CTGAN 4\n",
      "CTGAN\n",
      "TVAE 0\n",
      "TVAE 1\n",
      "TVAE 2\n",
      "TVAE 3\n",
      "TVAE 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [1:03:12<00:00, 421.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVAE\n",
      "protein\n",
      "{'CTGAN': {'discard': 0.42779999999999996, 'mae': 0.22943454709065453}, 'TVAE': {'discard': 0.5052000000000001, 'mae': 0.2090141668717247}}\n",
      "bio\n",
      "CTGAN 0\n",
      "[Errno 2] No such file or directory: './data/CASP.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_lists = [\n",
    "    \"bike\",\n",
    "    \"boston\",\n",
    "    \"concrete\",\n",
    "    \"star\",\n",
    "    \"mimic_antibiotics\",\n",
    "    \"los\",\n",
    "    \"cancer\",\n",
    "    \"protein\",\n",
    "    \"bio\",\n",
    "]\n",
    "n_runs = 5\n",
    "cal_size = 0.2\n",
    "\n",
    "\n",
    "final_results = {}\n",
    "\n",
    "for dataset in tqdm(dataset_lists):\n",
    "    print(dataset)\n",
    "    try:\n",
    "        results = {}\n",
    "\n",
    "        models = [CTGAN(), TVAE()]\n",
    "        modelnames = [\"CTGAN\", \"TVAE\"]\n",
    "\n",
    "        for idx in range(len(models)):\n",
    "            prop_discarded = []\n",
    "            mse = []\n",
    "            mae = []\n",
    "\n",
    "            for j in range(n_runs):\n",
    "\n",
    "                print(modelnames[idx], j)\n",
    "\n",
    "                seed = j * 10\n",
    "                random.seed(seed)\n",
    "                seed_everything(seed)\n",
    "\n",
    "                if dataset in uci_datasets:\n",
    "                    X_prop_train, y_prop_train = get_uci(dataset=dataset)\n",
    "                else:\n",
    "                    X_prop_train, y_prop_train = get_medical(dataset=dataset, seed=seed)\n",
    "\n",
    "                df = pd.DataFrame.from_records(\n",
    "                    X_prop_train\n",
    "                )  # pd.DataFrame(X_prop_train)\n",
    "                ints = list(df.columns)\n",
    "                df.columns = [str(x) for x in ints]\n",
    "\n",
    "                df[\"y\"] = y_prop_train\n",
    "\n",
    "                model = models[idx]\n",
    "\n",
    "                # Fit synthetic data model\n",
    "                model.fit(df)\n",
    "\n",
    "                # Sample data to train\n",
    "                nrows = 10000\n",
    "                sampled_data = model.sample(num_rows=nrows, randomize_samples=False)\n",
    "                X_prop_train = sampled_data.drop(columns=[\"y\"])\n",
    "                y_prop_train = sampled_data[\"y\"]\n",
    "\n",
    "                min_y = y_prop_train.min()\n",
    "                max_y = y_prop_train.max()\n",
    "\n",
    "                y_prop_train = np.array(\n",
    "                    [\n",
    "                        (y_prop_train[i] - min_y) / (max_y - min_y)\n",
    "                        for i in range(len(y_prop_train))\n",
    "                    ]\n",
    "                )\n",
    "                y_prop_train = pd.Series(y_prop_train)\n",
    "\n",
    "                if dataset in uci_datasets:\n",
    "                    X_eval, y_eval = get_uci(dataset=dataset)\n",
    "                else:\n",
    "                    X_eval, y_eval = get_medical(dataset=dataset, seed=seed)\n",
    "\n",
    "                y_eval = np.array(\n",
    "                    [(y_eval[i] - min_y) / (max_y - min_y) for i in range(len(y_eval))]\n",
    "                )\n",
    "                try:\n",
    "                    y_eval = pd.Series(y_eval)\n",
    "                except:\n",
    "                    y_eval = pd.Series(y_eval.reshape(-1))\n",
    "\n",
    "                test_ids = random.sample(\n",
    "                    list(range(len(y_eval))), int(0.5 * len(y_eval))\n",
    "                )\n",
    "                try:\n",
    "                    X_test = X_eval.iloc[test_ids, :]\n",
    "                except:\n",
    "                    X_test = X_eval[test_ids, :]\n",
    "\n",
    "                try:\n",
    "                    y_test = y_eval.iloc[test_ids]\n",
    "                except:\n",
    "                    y_test = y_eval[test_ids]\n",
    "\n",
    "                remaining_eval_ids = np.setdiff1d(range(len(y_eval)), test_ids)\n",
    "\n",
    "                try:\n",
    "                    _, X_cal, _, y_cal = train_test_split(\n",
    "                        X_eval.iloc[remaining_eval_ids, :],\n",
    "                        y_eval.iloc[remaining_eval_ids],\n",
    "                        test_size=cal_size,\n",
    "                        random_state=seed,\n",
    "                    )\n",
    "                except:\n",
    "                    _, X_cal, _, y_cal = train_test_split(\n",
    "                        X_eval[remaining_eval_ids, :],\n",
    "                        y_eval[remaining_eval_ids],\n",
    "                        test_size=cal_size,\n",
    "                        random_state=seed,\n",
    "                    )\n",
    "\n",
    "                X_prop_train, X_cal, X_test = (\n",
    "                    np.array(X_prop_train),\n",
    "                    np.array(X_cal),\n",
    "                    np.array(X_test),\n",
    "                )\n",
    "                y_prop_train, y_cal, y_test = (\n",
    "                    np.array(y_prop_train),\n",
    "                    np.array(y_cal),\n",
    "                    np.array(y_test),\n",
    "                )\n",
    "\n",
    "                prop = 0.1\n",
    "\n",
    "                num_ids = int(prop * len(y_prop_train))\n",
    "\n",
    "                last_ids = range(len(y_prop_train))\n",
    "\n",
    "                nest = 10\n",
    "                learner = xgb.XGBRegressor(n_estimators=nest, random_state=seed)\n",
    "                learner.fit(X_prop_train, y_prop_train)\n",
    "\n",
    "                y_eval = y_prop_train\n",
    "                X_eval = X_prop_train\n",
    "\n",
    "                triage = Triage(\n",
    "                    X_eval=X_eval,\n",
    "                    y_eval=y_eval,\n",
    "                    X_cal=X_cal,\n",
    "                    y_cal=y_cal,\n",
    "                    nest=nest,\n",
    "                    learner=learner,\n",
    "                )\n",
    "                groups_ids, raw_metrics = triage.run(\n",
    "                    compute_cpd=True, compute_crps=True\n",
    "                )\n",
    "                triage_array = raw_metrics[\"score_metric\"]\n",
    "\n",
    "                percentile_thresh = 75\n",
    "                thresh = 0.33\n",
    "                conf_thresh_low = thresh\n",
    "                conf_thresh_high = 1 - thresh\n",
    "                conf_thresh = 0.5\n",
    "\n",
    "                metric = triage_array\n",
    "\n",
    "                uncert = np.std(metric, axis=-1)\n",
    "                confidence = np.mean(metric, axis=-1)\n",
    "                # Get groups and mainly well-estimated groups\n",
    "                oe_group = np.where(\n",
    "                    (confidence <= conf_thresh_low)\n",
    "                    & (uncert <= np.percentile(uncert, percentile_thresh))\n",
    "                )[0]\n",
    "                ue_group = np.where(\n",
    "                    (confidence >= conf_thresh_high)\n",
    "                    & (uncert <= np.percentile(uncert, percentile_thresh))\n",
    "                )[0]\n",
    "                combined_group = np.concatenate((oe_group, ue_group))\n",
    "                we_group = []\n",
    "                for id in range(len(confidence)):\n",
    "                    if id not in combined_group:\n",
    "                        we_group.append(id)\n",
    "                we_group = np.array(we_group)\n",
    "\n",
    "                prop_discarded.append(\n",
    "                    (len(ue_group) + len(oe_group)) / len(X_prop_train)\n",
    "                )\n",
    "\n",
    "                y_pred = learner.predict(X_test)\n",
    "\n",
    "                from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "                mae.append(mean_absolute_error(y_pred, y_test))\n",
    "\n",
    "            print(modelnames[idx])\n",
    "            results[modelnames[idx]] = {\n",
    "                \"discard\": np.mean(prop_discarded),\n",
    "                \"mae\": np.mean(mae),\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    print(dataset)\n",
    "    print(results)\n",
    "    final_results[dataset] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston CTGAN: 41.0% 0.17 TVAE:  59.0% 0.12\n",
      "concrete CTGAN: 42.0% 0.18 TVAE:  49.0% 0.11\n",
      "cancer CTGAN: 48.0% 0.2 TVAE:  49.0% 0.18\n",
      "protein CTGAN: 56.99999999999999% 0.23 TVAE:  49.0% 0.21\n"
     ]
    }
   ],
   "source": [
    "for key in final_results.keys():\n",
    "    print(\n",
    "        key,\n",
    "        \"CTGAN:\",\n",
    "        f\"{np.round(1-final_results[key]['CTGAN']['discard'],2)*100}%\",\n",
    "        np.round(final_results[key][\"CTGAN\"][\"mae\"], 2),\n",
    "        \"TVAE: \",\n",
    "        f\"{np.round(1-final_results[key]['TVAE']['discard'],2)*100}%\",\n",
    "        np.round(final_results[key][\"TVAE\"][\"mae\"], 2),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fHY_IsnvhUf",
    "outputId": "d2b597b2-d9a9-425f-a84f-7608c9edd554"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boston': {'CTGAN': {'discard': 0.59248, 'mae': 0.16940937866560452},\n",
       "  'TVAE': {'discard': 0.40671999999999997, 'mae': 0.12299813702623756}},\n",
       " 'concrete': {'CTGAN': {'discard': 0.5768600000000002,\n",
       "   'mae': 0.18450783407877333},\n",
       "  'TVAE': {'discard': 0.5128600000000001, 'mae': 0.11010332626540328}},\n",
       " 'cancer': {'CTGAN': {'discard': 0.51586, 'mae': 0.19781326705459215},\n",
       "  'TVAE': {'discard': 0.50706, 'mae': 0.182426155038921}},\n",
       " 'protein': {'CTGAN': {'discard': 0.42779999999999996,\n",
       "   'mae': 0.22943454709065453},\n",
       "  'TVAE': {'discard': 0.5052000000000001, 'mae': 0.2090141668717247}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
